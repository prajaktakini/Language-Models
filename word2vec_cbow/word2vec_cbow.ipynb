{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"13miWGLAxnhkYebSi9Qpk-s0qnPAsihMy","authorship_tag":"ABX9TyO3Pf0qj3Eg9ieH9kuUyGRL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["\n","\n","> https://github.com/oliverquintana/CBOWWordPrediction/blob/main/NextWordPredictionCBOW.ipynb\n"],"metadata":{"id":"_Wv1MCZNErrg"}},{"cell_type":"markdown","source":["# Pre-Requisites"],"metadata":{"id":"paSByfZAIG_b"}},{"cell_type":"code","source":["!pip install ipynb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KtGSOF6E2RUT","executionInfo":{"status":"ok","timestamp":1696936403332,"user_tz":-330,"elapsed":13011,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}},"outputId":"c32ab1cb-2591-4529-cb33-01bf06446ab5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ipynb\n","  Downloading ipynb-0.5.1-py3-none-any.whl (6.9 kB)\n","Installing collected packages: ipynb\n","Successfully installed ipynb-0.5.1\n"]}]},{"cell_type":"markdown","source":["\n","How to read one ipynb file from another in google colab\n","1.   https://www.pingshiuanchua.com/blog/post/importing-your-own-python-module-or-python-file-in-colaboratory\n","2.   https://saturncloud.io/blog/importing-its-own-ipynb-files-on-google-colab/\n","3. https://stackoverflow.com/questions/53254703/import-its-own-ipynb-files-on-google-colab\n","\n"],"metadata":{"id":"Q6jNxLJD3G5D"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qsc6ukBD2zjc","executionInfo":{"status":"ok","timestamp":1696936429895,"user_tz":-330,"elapsed":26572,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}},"outputId":"1184c0c6-314e-44f2-cc51-48260a96a571"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Word2Vec_CBOW')"],"metadata":{"id":"Bi8NM40B27KG","executionInfo":{"status":"ok","timestamp":1696936429895,"user_tz":-330,"elapsed":6,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# This is to read utils file from this file\n","%run /content/drive/MyDrive/Colab\\ Notebooks/Word2Vec_CBOW/utils.ipynb"],"metadata":{"id":"6_P1nUjM3amE","executionInfo":{"status":"ok","timestamp":1696936431159,"user_tz":-330,"elapsed":1268,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"rZsNjiZFWMGd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696936458838,"user_tz":-330,"elapsed":27687,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}},"outputId":"13084219-fe32-4bb3-bd79-e55e7f8b54db"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}],"source":["# Load Dependencies\n","import json\n","import nltk\n","import spacy\n","import numpy as np\n","import tensorflow as tf\n","\n","from nltk.tokenize import RegexpTokenizer\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"code","source":["!python -m spacy download en_core_web_sm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LH90EyfW0jMQ","executionInfo":{"status":"ok","timestamp":1696936478899,"user_tz":-330,"elapsed":20069,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}},"outputId":"f9df42d7-7f1a-4cb3-fcee-dc90bef66ed3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-10-10 11:14:23.369123: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-sm==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"code","source":["# Load the corpus\n","data = read_file('the_film.txt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":462},"id":"dPN1i0YI0jvE","executionInfo":{"status":"error","timestamp":1696936479596,"user_tz":-330,"elapsed":708,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}},"outputId":"4c56b969-fdfc-4032-e732-12442555458e"},"execution_count":7,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-21e364a1ab63>\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'the_film.txt'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-61edeb4cf1f9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'the_film.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-21e364a1ab63>\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mdata_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mdetected_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchardet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'the_film.txt'"]}]},{"cell_type":"markdown","source":["# Data Pre-Processing"],"metadata":{"id":"TZ5ChKS7IKzn"}},{"cell_type":"code","source":["# Tokenization\n","regex_tokenizer = RegexpTokenizer(r'\\w+')\n","tokenized_string = regex_tokenizer.tokenize(data)\n","print(f'Tokenized corpus {tokenized_string}')"],"metadata":{"id":"e7ZgPQSE4rls","executionInfo":{"status":"aborted","timestamp":1696936479597,"user_tz":-330,"elapsed":14,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data pre-processing\n","print(f'Input tokens size {len(tokenized_string)}')\n","\n","# 1. Remove punctuation\n","import string\n","\n","for i, token in enumerate(tokenized_string):\n","  new_token = ''\n","  for char in token:\n","    if char not in string.punctuation:\n","      new_token += char\n","  tokenized_string[i] = new_token\n","\n","# 2. Remove numerics\n","itr = len(tokenized_string) - 1\n","\n","while itr >= 0:\n","  if tokenized_string[itr] == '':\n","    tokenized_string.pop(itr)\n","  else:\n","    for char in tokenized_string[itr]:\n","      try:\n","        int(char) # If token contains numberic literal, remove that token\n","        tokenized_string.pop(itr)\n","      except:\n","        continue\n","  itr -= 1\n","\n","# 3. Remove tokens with len < 2\n","itr = len(tokenized_string) - 1\n","\n","while itr >= 0:\n","  if len(tokenized_string[itr]) < 2:\n","       tokenized_string.pop(itr)\n","\n","  itr -= 1\n","\n","# 4. Remove stop words\n","from nltk.corpus import stopwords\n","\n","stopwords_set = set(stopwords.words('english'))\n","tokenized_string = [token for token in tokenized_string if token not in stopwords_set]\n","\n","\n","print(f'Output tokens size {len(tokenized_string)}')"],"metadata":{"id":"XB4nOZTN5qo6","executionInfo":{"status":"aborted","timestamp":1696936479597,"user_tz":-330,"elapsed":14,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lemmitization\n","# https://spacy.io/models/en#en_core_web_sm\n","lemmitizer = spacy.load('en_core_web_sm')\n","for i in range(len(tokenized_string)):\n","  tokenized_string[i] = tokenized_string[i].lower()\n","  tokens = lemmitizer(tokenized_string[i])\n","  lemmas = [token.lemma_.lower() for token in tokens]\n","  tokenized_string[i] = lemmas[0]\n","\n","  if i % 500 == 0:\n","    print(\"Progress check in {} / {}\".format(i, tokenized_string[i]))\n"],"metadata":{"id":"KZSg39QI5qm8","executionInfo":{"status":"aborted","timestamp":1696936479597,"user_tz":-330,"elapsed":10,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preview Corpus\n","for _ in range(10):\n","  print(tokenized_string[np.random.randint(len(tokenized_string))])"],"metadata":{"id":"FLK8YSPN5qk-","executionInfo":{"status":"aborted","timestamp":1696936479598,"user_tz":-330,"elapsed":11,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save clean corpus to separate file\n","output_file = ''\n","file = open('preprocessed_corpus.txt', 'w')\n","for i, word in enumerate(tokenized_string):\n","  temp = word + ' '\n","  file.write(temp)\n","file.close()"],"metadata":{"id":"Hm9LsAv7XCqx","executionInfo":{"status":"aborted","timestamp":1696936479598,"user_tz":-330,"elapsed":11,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenized_string)"],"metadata":{"id":"BmOT5SlFHKIU","executionInfo":{"status":"aborted","timestamp":1696936479598,"user_tz":-330,"elapsed":11,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save vocab to separate file\n","vocab = create_vocab(tokenized_string)\n","with open('vocabulary.txt', mode = 'w') as output_file:\n","  json.dump(vocab, output_file)\n"],"metadata":{"id":"oG0OAoZy5qiq","executionInfo":{"status":"aborted","timestamp":1696936479598,"user_tz":-330,"elapsed":10,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Continuous Bag Of Words Model"],"metadata":{"id":"fzeCuCdOIBrv"}},{"cell_type":"code","source":["class CBOW:\n","  def __init__(self, vocab_size, context_size, num_epochs = 100, learning_rate = 0.001):\n","    self.context_size = context_size\n","\n","    # Initialise model\n","    self.model = Sequential() # https://www.tensorflow.org/guide/keras/sequential_model\n","\n","    # Define layers\n","    self.model.add(Dense(100, input_dim = vocab_size)) # https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n","    self.model.add(Dense(vocab_size, activation = \"softmax\"))\n","\n","    # Initialise optimizer. We are using Adam optimizer\n","    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n","\n","    self.model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer)\n","\n","    self.model.summary()\n","\n","\n","  def update_learning_rate(learning_rate = 0.001):\n","    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n","    self.model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer)\n","\n","\n","  def train_model(self, corpus, vocab, num_epochs = 10, batch_size = 10, file_name = 'cbow.h5'):\n","\n","    def get_context_words(corpus, vocab, batch_size):\n","      indices = np.random.randint(self.context_size, len(corpus) - self.context_size, batch_size) # Why?\n","\n","      X = np.zeros([batch_size, self.context_size, len(list(vocab.keys()))])\n","      Y = np.zeros([batch_size, len(list(vocab.keys()))])\n","\n","      for i, index in enumerate(indices):\n","        context = []\n","        word = corpus[index]\n","        context.extend(corpus[index - self.context_size : index]) # Context before given word\n","        Y[i, vocab[word]] = 1\n","\n","        for j, context_word in enumerate(context):\n","          X[i, j, vocab[word]] = 1\n","\n","      return X, Y\n","\n","\n","    steps = int(np.floor(len(corpus) - self.context_size / batch_size))\n","\n","    for epoch in range(num_epochs):\n","      for step in range(steps):\n","        X_batch, Y_batch = get_context_words(corpus, vocab, batch_size)\n","        X_batch = np.sum(X_batch, axis = 1)\n","        loss = self.model.train_on_batch(X_batch, Y_batch)\n","\n","        print('Epoch: {}/{} Step: {}/{} Loss: {}'.format(epoch, num_epochs, step, steps, loss))\n","      self.model.save(file_name)\n","\n","    return\n","\n","\n","  def predict(self, indices, vocab, num_predictions = 3):\n","    vocab_words = list(vocab.keys())\n","\n","    X = np.zeros([len(indices), len(vocab_words)], dtype = 'ushort')\n","    for i, index in enumerate(indices):\n","      X[i, index] = 1\n","\n","    prediction = self.model.predict(X)\n","\n","    dict_predictions = {}\n","\n","    for i in range(prediction.shape[0]):\n","      word_predictions = []\n","      for _ in range(num_predictions):\n","        index = np.argmax(prediction[i])\n","\n","        word = vocab[index]\n","        prob = prediction[i, index]\n","        word_predictions.append([word, prob])\n","        prediction[i, index] = 0\n","\n","      dict_predictions[vocab[indices[i]]] = word_predictions\n","\n","    for key, val in dict_predictions.items():\n","      s = ''\n","      for x in val:\n","        s += x[0] + '-' + str(np.round(x[1] * 100, 3)) + '%' + ' '\n","      print('Context: {} Predictions: {}'.format(key, s))\n","\n","    return"],"metadata":{"id":"vVPWg9_u5qfz","executionInfo":{"status":"aborted","timestamp":1696936479598,"user_tz":-330,"elapsed":9,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load corpus\n","preprocessed_corpus = read_file('preprocessed_corpus.txt')\n","\n","tokenizer = RegexpTokenizer(r'\\w+')\n","tokens = tokenizer.tokenize(preprocessed_corpus)\n","\n","stop_words = nltk.corpus.stopwords.words('english')\n","tokens = [token for token in tokens if token not in stop_words]\n","\n","with open('vocabulary.txt') as json_file:\n","  vocab = json.load(json_file)\n"],"metadata":{"id":"aH1_rYKb5qYH","executionInfo":{"status":"aborted","timestamp":1696936479598,"user_tz":-330,"elapsed":8,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build CBOW model\n","context_size = 1                     # Context size\n","vocab_size = len(list(vocab.keys())) # Vocabulary size\n","learning_rate = 0.001\n","\n","# Build a model\n","model = CBOW(vocab_size = vocab_size, context_size = context_size, learning_rate = learning_rate)\n"],"metadata":{"id":"Po2Ae6pLYc8i","executionInfo":{"status":"aborted","timestamp":1696936479598,"user_tz":-330,"elapsed":8,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load pre-trained model weights\n","#model.model = tf.keras.models.load_model('cbow.h5')\n","\n","# Initiate model training\n","model.train_model(tokens, vocab, num_epochs = 100, batch_size = 5000)"],"metadata":{"id":"YnjZ51v1Y-0I","executionInfo":{"status":"aborted","timestamp":1696936479598,"user_tz":-330,"elapsed":8,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Word prediction\n","sample_size = 5 # Number of examples to predict\n","num_predictions = 3 # Predictions per sample\n","\n","indices = np.random.randint(0, len(list(vocab.keys())), sample_size)\n"],"metadata":{"id":"KuOmA_WvZgu9","executionInfo":{"status":"aborted","timestamp":1696936479599,"user_tz":-330,"elapsed":9,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save trained model\n","model.model.save('cbow.h5')\n","#del model"],"metadata":{"id":"3sWlBJm6dg72","executionInfo":{"status":"aborted","timestamp":1696936479599,"user_tz":-330,"elapsed":9,"user":{"displayName":"Prajakta Kini","userId":"04705995834041182860"}}},"execution_count":null,"outputs":[]}]}