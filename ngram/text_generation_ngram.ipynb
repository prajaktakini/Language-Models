{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFD34jnz95c2M+Ek19uK75",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prajaktakini/Language-Models/blob/main/text_generation_ngram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "References:\n",
        "1. Medium Article https://towardsdatascience.com/text-generation-using-n-gram-model-8d12d9802aa0\n",
        "2. Linkedin Blog https://www.linkedin.com/pulse/evolution-language-models-my-notes-ajay-taneja/?trackingId=kp09TQv9RRqE2QTC%2FOLL%2FA%3D%3D\n",
        "\n",
        "Links from where EBooks can be downloaded: https://www.gutenberg.org/\n",
        "# Book that is used in this implementation: https://www.gutenberg.org/ebooks/71747\n",
        "```"
      ],
      "metadata": {
        "id": "w5ge0AjMbQ2_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Dqdzi6hkCMO-"
      },
      "outputs": [],
      "source": [
        "# All imports\n",
        "import string\n",
        "import random\n",
        "import time\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Below is the tokenizer function that tokenizes given text into tokens\n",
        "def tokenize(text: str) -> List[str]:\n",
        "  '''\n",
        "  text -> takes in input string to be tokenized\n",
        "  returns list of string tokens post tokenization\n",
        "  '''\n",
        "  for punctuation in string.punctuation:\n",
        "    text = text.replace(punctuation, ' ' + punctuation + ' ') # Add spaces before and after punctuations\n",
        "  tokens = text.split()\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "15kGChFHCYZg"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample result returned from tokenize function\n",
        "#print(tokenize(\"I am sorry, I made mistake\"))"
      ],
      "metadata": {
        "id": "wRA7OhM3CYQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef43730-7990-4f25-f228-2212c9365c5b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'sorry', ',', 'I', 'made', 'mistake']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_ngrams(n: int, tokens:list) -> list:\n",
        "  '''\n",
        "  n -> defines size of n-gram\n",
        "  tokens -> list of tokens\n",
        "  returns the list of n-grams\n",
        "  '''\n",
        "\n",
        "  # This is to avoid the missing tokens problem when the sentence starts eg. Say, our sentence is I love reading books , for 3 grams, initial consideration would be (<START>, <START>, I) and so on\n",
        "  tokens = (n - 1) * [\"<START>\"] + tokens\n",
        "\n",
        "  l = [(tuple([tokens[i-p-1] for p in reversed(range(n-1))]), tokens[i]) for i in range(n-1, len(tokens))] # Creates all tuples such as (('<START>', '<START>'), 'I'), (('<START>', 'I'), 'am') and so on\n",
        "  return l\n",
        ""
      ],
      "metadata": {
        "id": "obtH9YwRCYIH"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample result returned from find_ngrams function\n",
        "#print(find_ngrams(3, ['I', 'am', 'sorry', ',', 'I', 'made', 'mistake']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTCscWVaU1Z2",
        "outputId": "c2071178-cd9d-4bd5-f793-5dc2424132b6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<START>', '<START>', 'I', 'am', 'sorry', ',', 'I', 'made', 'mistake']\n",
            "[(('<START>', '<START>'), 'I'), (('<START>', 'I'), 'am'), (('I', 'am'), 'sorry'), (('am', 'sorry'), ','), (('sorry', ','), 'I'), ((',', 'I'), 'made'), (('I', 'made'), 'mistake')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Ngram(object):\n",
        "\n",
        "  def __init__(self, n):\n",
        "    # n defines n-gram count\n",
        "    self.n = n\n",
        "\n",
        "    # dictionary that keeps track of list of possible words given a context\n",
        "    self.context = {}\n",
        "\n",
        "    # counter that keeps track of how many times ngram has appeared in the corpus\n",
        "    self.ngram_counter = {}\n",
        "\n",
        "\n",
        "  # Updates the language model\n",
        "  def update(self, sentence: str) -> None:\n",
        "    n = self.n\n",
        "\n",
        "    ngrams = find_ngrams(n, tokenize(sentence))\n",
        "\n",
        "    # Increment ngram counter in the dictionary\n",
        "    for ngram in ngrams:\n",
        "      if ngram in self.ngram_counter:\n",
        "        self.ngram_counter[ngram] += 1\n",
        "      else:\n",
        "        self.ngram_counter[ngram] = 1\n",
        "\n",
        "      prev_words, target_word = ngram # prev_words -> ('I', 'love') and target_word -> 'reading'\n",
        "\n",
        "      # Add the target word to the list of prev words\n",
        "      if prev_words in self.context:\n",
        "        self.context[prev_words].append(target_word)\n",
        "      else:\n",
        "        self.context[prev_words] = [target_word]\n",
        "\n",
        "\n",
        "  # Given a context dictionary, finds the probability of this token to be generated. Returns a conditional probability\n",
        "  # https://www.linkedin.com/pulse/evolution-language-models-my-notes-ajay-taneja/?trackingId=kp09TQv9RRqE2QTC%2FOLL%2FA%3D%3D\n",
        "  def find_token_probability(self, context, token) -> float:\n",
        "\n",
        "    try:\n",
        "      count_of_ngram = self.ngram_counter[(context, token)] # gives frequency of this n-gram\n",
        "      number_of_possible_targets = len(self.context[context])\n",
        "      result = float(count_of_ngram) / float(number_of_possible_targets)\n",
        "    except KeyError:\n",
        "      result = 0.0\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "  # Given a context, generate a next word to append in sequence semi-randomly\n",
        "  def generate_random_token(self, context):\n",
        "\n",
        "    r = random.random()\n",
        "    token_prob_map = {} # holds each token with its probability for a given context\n",
        "    possible_tokens = self.context[context]\n",
        "\n",
        "    for token in possible_tokens:\n",
        "      token_prob_map[token] = self.find_token_probability(context, token)\n",
        "\n",
        "    sum = 0\n",
        "    for token in sorted(token_prob_map):\n",
        "      sum += token_prob_map[token]\n",
        "\n",
        "      if sum > r:\n",
        "        return token\n",
        "\n",
        "\n",
        "  # generates text for the given count of tokens\n",
        "  def generate_text(self, token_count: int):\n",
        "    '''\n",
        "    token_count -> defines number of tokens to be generated\n",
        "    '''\n",
        "\n",
        "    n = self.n\n",
        "\n",
        "    # provide some context to the system by prepending (n - 1) '<START>' tokens\n",
        "    context_queue = (n - 1) * ['<START>']\n",
        "    result = []\n",
        "\n",
        "    for _ in range(token_count):\n",
        "      obj = self.generate_random_token(tuple(context_queue))\n",
        "      result.append(obj)\n",
        "      if n > 1:\n",
        "        context_queue.pop(0)\n",
        "\n",
        "        # when we reached the full stop . in generation, the system would be not aware of how to proceed, because of the way how we were updating our Language Model.\n",
        "        # Therefore, we would need to reinitialize the contextual queue context_queue every time our model generates a full stop.\n",
        "        if obj == '.':\n",
        "          context_queue = (n - 1) * [\"<START>\"]\n",
        "        else:\n",
        "          context_queue.append(obj)\n",
        "\n",
        "    return ' '.join(result)\n"
      ],
      "metadata": {
        "id": "oCnzRdvlVVsX"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialises the n-gram model and dictionaries\n",
        "def create_ngram_model(n, path):\n",
        "  model = Ngram(n)\n",
        "  with open(path, 'r') as f:\n",
        "    text = f.read()\n",
        "    text = text.split('.') # split sentences\n",
        "    for sentence in text:\n",
        "      sentence += '.' # Add back fullstop\n",
        "      model.update(sentence) # Update the dictionary\n",
        "  return model\n",
        "\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "  start = time.time()\n",
        "  model = create_ngram_model(3, '/content/sample_data/the_film.txt')\n",
        "\n",
        "  print(f'Time taken to create ngram model: {time.time() - start}')\n",
        "\n",
        "  start = time.time()\n",
        "  random.seed(5)\n",
        "\n",
        "  print(f'{\"=\"* 50}\\nGenerated text')\n",
        "  print(model.generate_text(20))\n",
        "  print(f'{\"=\" * 50}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U27UsputmLHb",
        "outputId": "69df6518-ca5e-4421-bd33-e51f2c1a5464"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to create ngram model: 0.2583951950073242\n",
            "==================================================\n",
            "Generated text\n",
            "Take a novel as an indispensable ally of the nation makes of this agreement and help preserve free future access\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}